{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'text': [\n",
    "        'Я люблю программирование.',\n",
    "        'Погода сегодня прекрасная.',\n",
    "        'Этот фильм был ужасен.',\n",
    "        'Я обожаю читать книги.',\n",
    "        'Спорт - это здорово!'\n",
    "    ],\n",
    "    'label': [1, 1, 0, 1, 1]  # 1 - положительный, 0 - отрицательный\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('rus_text_classification.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rus_text_classification.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings = tokenizer.batch_encode_plus(train_texts, \n",
    "                                              add_special_tokens=True, \n",
    "                                              max_length=512, \n",
    "                                              padding='max_length', \n",
    "                                              truncation=True, \n",
    "                                              return_attention_mask=True, \n",
    "                                              return_tensors='pt')\n",
    "\n",
    "val_encodings = tokenizer.batch_encode_plus(val_texts, \n",
    "                                             add_special_tokens=True, \n",
    "                                             max_length=512, \n",
    "                                             padding='max_length', \n",
    "                                             truncation=True, \n",
    "                                             return_attention_mask=True, \n",
    "                                             return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels.values))\n",
    "val_dataset = torch.utils.data.TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(val_labels.values))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Потери: 0.6608932614326477\n",
      "Эпоха 1, Точность на валидации: 1.0000\n",
      "Эпоха 2, Потери: 0.6667951345443726\n",
      "Эпоха 2, Точность на валидации: 1.0000\n",
      "Эпоха 3, Потери: 0.6120964884757996\n",
      "Эпоха 3, Точность на валидации: 1.0000\n",
      "Эпоха 4, Потери: 0.5779806971549988\n",
      "Эпоха 4, Точность на валидации: 1.0000\n",
      "Эпоха 5, Потери: 0.5392158627510071\n",
      "Эпоха 5, Точность на валидации: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Эпоха {epoch+1}, Потери: {total_loss / len(train_loader)}')\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    accuracy = total_correct / len(val_loader.dataset)\n",
    "    print(f'Эпоха {epoch+1}, Точность на валидации: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанная метка: 1\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Я люблю машинное обучение.\"\n",
    "test_encoding = tokenizer.encode_plus(test_text, \n",
    "                                       add_special_tokens=True, \n",
    "                                       max_length=512, \n",
    "                                       padding='max_length', \n",
    "                                       truncation=True, \n",
    "                                       return_attention_mask=True, \n",
    "                                       return_tensors='pt')\n",
    "\n",
    "input_ids, attention_mask = test_encoding['input_ids'], test_encoding['attention_mask']\n",
    "\n",
    "output = model(input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "_, predicted = torch.max(output.logits, 1)\n",
    "print(f'Предсказанная метка: {predicted.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
